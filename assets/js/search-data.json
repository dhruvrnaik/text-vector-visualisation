{
  
    
        "post0": {
            "title": "Visualising Word Vectors Using TF2 [Advisable]",
            "content": "Tensorflow released their latest version of TensorFlow 2 on September 30, 2019. . As also mentioned in tb1vis.ipynb the reason we are visualising FastText is because: . FastText uses the concept that embeddings are formed based on the sub-word approach, this method helps us to visualise and obtain misspellings of a word or different spellings of the same word.I couldn&#39;t find any blogs on the Internet that have covered or updated their code which describes the visualisation of embeddings through the latest version of Tensorflow. . Although I did take the help of an issue TF 2.0 API for using the embedding projector raised on the Tensorflow repository and have come to a concluding notebook suiting my goal i.e. to visualise FastText embeddings using TF2. . The tensorflow version used in this notebook is version 2. . . # import statements from pathlib import PurePath import fasttext import numpy as np import tensorflow as tf from tensorflow.python.framework import ops from tensorboard.plugins import projector from tensorboard.plugins.projector import ProjectorConfig . #hide_output # load pre-trained fasttext model model = fasttext.load_model(&quot;fasttextmodel.bin&quot;) . for i, w in enumerate(model.get_words()): print(w) if i &gt; 4: break . s said mr &lt;/s&gt; people new . #hide_output # number of words in the dataset VOCAB_SIZE = len(model.get_words()) # size of the dimension of each word vector EMBEDDING_DIM = len(model.get_word_vector(w)) # 2D numpy array initialised to store words with their vector representation embed = np.zeros((VOCAB_SIZE, EMBEDDING_DIM)) embed.shape . # store the vector representation of each word in the 2D numpy array for i, word in enumerate(model.get_words()): embed[i] = model.get_word_vector(word) embed . array([[-0.11363645, 0.00304414, 0.00589875, ..., 0.00278742, 0.03564256, -0.10496949], [ 0.05821591, 0.07343163, -0.06941246, ..., 0.00737938, 0.08668958, -0.05127012], [ 0.06867523, -0.02112868, -0.02132288, ..., 0.05362611, 0.13982825, 0.04221647], ..., [ 0.16511762, 0.04439345, -0.14276202, ..., 0.02632121, 0.03970968, 0.03706815], [ 0.09471416, 0.09356211, 0.00358974, ..., -0.0174412 , 0.13414964, 0.02268019], [ 0.07753251, -0.02356024, -0.05303693, ..., 0.14130574, 0.09740689, 0.0418443 ]]) . # path to store the words tsv_file_path = &quot;tensorboard/metadata.tsv&quot; . . Projection on Tensorboard 2 . Steps for projection: . Define the function register_embedding() and save_label_tsv() to configure the projector as well as save the projector configuration files and metadata file to the same folder. | Initialise the path variables accordingly and call the above function with suitable path variables as shown in below cells. | Creation of the tensorflow variable instead of tensorflow placeholder. | A saver class object is initialised and checkpoint is created. | . Differences between TF 1 and TF 2 . We cannot call the reset default graph method directly from tf library which we did for TensorFlow 1. It is invoked in TF2 as: | python from tensorflow.python.framework import ops ops.reset_default_graph() There is no placeholder required here. A tf variable is created which is passed parameters. A TF variable is shown below. Here the parameters are x: the array which contains the embeddings and name: name of the embedding file. | python tensor_embeddings = tf.Variable(x, name=EMBEDDINGS_TENSOR_NAME) According to the TF2 documentation a tensorflow variable is defined as: A variable maintains shared, persistent state manipulated by a program. The Variable() constructor requires an initial value for the variable, which can be a Tensor of any type and shape. This initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods. There is no concept of `session` as well as `saver` in TF2 yet. To workaround this, we just use the `saver` class for the creation of checkpoints by initialising the `saver` object with the tensorflow variable and pass `None` as value to the `session` parameter in `saver.save()`. | python saver = tf.compat.v1.train.Saver([tensor_embeddings]) saver.save(sess=None, global_step=STEP, save_path=EMBEDDINGS_FPATH) . . ops.reset_default_graph() # clearing the default graph stack def register_embedding( embedding_tensor_name: str, meta_data_fname: str, log_dir: str, ) -&gt; None: &quot;&quot;&quot; Configuring the projector to be read by the tensorboard. Args: embedding_tensor_name(str): embeddings file name meta_data_fname(str): metadata file name log_dir(str): folder where tensorboard files and the metadata file are saved Returns: None &quot;&quot;&quot; config = projector.ProjectorConfig() embedding = config.embeddings.add() embedding.tensor_name = embedding_tensor_name embedding.metadata_path = meta_data_fname projector.visualize_embeddings( log_dir, config ) # storing the configuration files of projector where tensorboard files are saved . def save_labels_tsv(labels: list, filepath: str, log_dir: str,) -&gt; None: &quot;&quot;&quot; Storing the vocabulary of words in the dataset to a file Args: labels: vocabulary i.e. words in the dataset filepath: metadata file name log_dir: &quot;folder where tensorboard files and projector files are saved Returns: None &quot;&quot;&quot; with open(PurePath(log_dir, filepath), &quot;w&quot;) as f: for label in labels: f.write(&quot;{} n&quot;.format(label)) . LOG_DIR = &quot;tb2files&quot; # folder which will contain all the tensorboard log files # Labels i.e. the words in the dataset will be stored in this file META_DATA_FNAME = &quot;meta.tsv&quot; # name of the file which will have the embeddings stored EMBEDDINGS_TENSOR_NAME = &quot;embeddings&quot; # path for checkpoint of the saved embeddings EMBEDDINGS_FPATH = PurePath(LOG_DIR, EMBEDDINGS_TENSOR_NAME + &quot;.ckpt&quot;) STEP = 0 x = embed # array containing the embeddings y = model.get_words() # list containing the vocabulary register_embedding(EMBEDDINGS_TENSOR_NAME, META_DATA_FNAME, LOG_DIR) save_labels_tsv(y, META_DATA_FNAME, LOG_DIR) . tensor_embeddings = tf.Variable( x, name=EMBEDDINGS_TENSOR_NAME ) # creation of the tensorflow variable, x: array which contains the embeddings, # name: name of the file which will have the embeddings stored . #hide_output saver = tf.compat.v1.train.Saver( [tensor_embeddings] ) # Tensorflow variable passed as argument for saver object to be initialised saver.save( sess=None, global_step=STEP, save_path=EMBEDDINGS_FPATH ) # saving the checkpoint for the embedding files . .",
            "url": "https://rohetoric.github.io/text-vector-visualisation/jupyter/tensorflow/python/2020/05/21/tb2vis.html",
            "relUrl": "/jupyter/tensorflow/python/2020/05/21/tb2vis.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Visualising Word Vectors Using TF1 [Not Advisable]",
            "content": "The visualisation of embeddings using Tensorflow 1 is inspired from this blog. . The above mentioned blog gives out an appropriate theoretical description but their code has become obsolete and the required changes for the correct visualisation as well as the complete theoretical description is given in this notebook and in an updated way with TF2 blog-post. . FastText uses the concept that embeddings are formed based on the sub-word approach, this method helps us to visualise and obtain misspellings of a word or different spellings of the same word.As we currently have the latest Tensorflow version installed, instead of downgrading it to previous version 1, we use the following code: . import tensorflow.compat.v1 as tf tf.disable_v2_behavior() . This method helps us to get the behaviour of Tensorflow 1 in Tensorflow 2. . . #hide_output from pathlib import PurePath import fasttext import numpy as np from tensorflow.python.framework import ops from tensorboard.plugins import projector import tensorflow.compat.v1 as tf tf.disable_v2_behavior() # disabling v2 behaviour of tf1 from tensorboard.plugins.projector import ProjectorConfig ops.reset_default_graph() . #hide_output model = fasttext.load_model(&quot;fasttextmodel.bin&quot;) . # directory to save files to visualise on tensorboard FOLDER_PATH = &quot;tb1files&quot; . for i, w in enumerate(model.get_words()): print(w) if i &gt; 4: break . s said mr &lt;/s&gt; people new . #hide_output # number of words in the dataset VOCAB_SIZE = len(model.get_words()) # size of the dimension of each word vector EMBEDDING_DIM = len(model.get_word_vector(w)) # 2D numpy array initialised to store words with their vector representation embed = np.zeros((VOCAB_SIZE, EMBEDDING_DIM)) embed.shape . # store the vector representation of each word in the 2D numpy array for i, word in enumerate(model.get_words()): embed[i] = model.get_word_vector(word) embed . array([[-0.11363645, 0.00304414, 0.00589875, ..., 0.00278742, 0.03564256, -0.10496949], [ 0.05821591, 0.07343163, -0.06941246, ..., 0.00737938, 0.08668958, -0.05127012], [ 0.06867523, -0.02112868, -0.02132288, ..., 0.05362611, 0.13982825, 0.04221647], ..., [ 0.16511762, 0.04439345, -0.14276202, ..., 0.02632121, 0.03970968, 0.03706815], [ 0.09471416, 0.09356211, 0.00358974, ..., -0.0174412 , 0.13414964, 0.02268019], [ 0.07753251, -0.02356024, -0.05303693, ..., 0.14130574, 0.09740689, 0.0418443 ]]) . # path to store the words tsv_file_path = FOLDER_PATH + &quot;/metadata.tsv&quot; . tsv_file_path . &#39;tb1files/metadata.tsv&#39; . with open(tsv_file_path, &quot;w+&quot;, encoding=&quot;utf-8&quot;) as f: for i, word in enumerate(model.get_words()): f.write(word + &quot; n&quot;) # write the words to an external file embed.shape . (10891, 300) . TENSORBOARD_FILES_PATH = FOLDER_PATH . . Projection on Tensorboard 1 [Part 1] . Steps for projection [Part 1]: . Placeholder is created of size Vocab Size * Dimension of Embeddings. | Creation of a global variable to store the placeholder values. | New tensorflow session is started and the placeholder is passed the value of our array which stores the vocabulary and their respective embeddings. | For saving values into variables and restoring variables from checkpoints, a saver object is instantiated and a writer object is initialised which outputs the graph. | . Differences between TF 1 and TF 2 . In TF1, reset default graph can be directly called by the tensorflow library to clear the default graph stack and reset the global default graph. Implemented by - | python tf.reset_default_graph() TF 2 doesn&#39;t have the placeholders as mentioned below: | python X_init = tf.placeholder(tf.float32, shape=(VOCAB_SIZE, EMBEDDING_DIM), name=&quot;embedding&quot;) It is cleared off by the disabling tf 2 behaviour defined through the import technique- python import tensorflow.compat.v1 as tf tf.disable_v2_behavior() If the import technique isn&#39;t followed we are subjected to receive the error:`AttributeError: module &#39;tensorflow&#39; has no attribute &#39;placeholder.&#39;` . . # Tensorflow Placeholders tf.reset_default_graph() X_init = tf.placeholder(tf.float32, shape=(VOCAB_SIZE, EMBEDDING_DIM), name=&quot;embedding&quot;) X = tf.Variable(X_init) # Initializer init = tf.global_variables_initializer() # Start Tensorflow Session sess = tf.Session() sess.run(init, feed_dict={X_init: embed}) # Instance of Saver, save the graph. saver = tf.train.Saver() writer = tf.summary.FileWriter(TENSORBOARD_FILES_PATH, sess.graph) . . Projection on Tensorboard 1 [Part 2] . Steps for projection [Part 2]: . Instantiating the projector object. | Assigning the file which contains the vocabulary to the embedding variable. | Writing the configuration file for the projector read by the tensorboard using projector.visualize_embeddings(writer, config) | Saving the checkpoint and closing the connection. | Here both the projector imports are important [already imported in cell 1] i.e. . from tensorboard.plugins import projector from tensorboard.plugins.projector import ProjectorConfig as visualize_embeddings() function is defined under projector and we need ProjectorConfig() in creation for the configuration file of the projector. . If the projector is imported in the following way: from tensorboard.plugins import projector config = projector.ProjectorConfig() the error received would be AttributeError: module &#39;tensorboard.plugins.projector&#39; has no attribute &#39;ProjectorConfig.&#39; . . # Configure a Tensorflow Projector config = ProjectorConfig() embed = config.embeddings.add() embed.metadata_path = &quot;metadata.tsv&quot; # Write a projector_config projector.visualize_embeddings(writer, config) # save a checkpoint saver.save(sess, TENSORBOARD_FILES_PATH + &quot;/model.ckpt&quot;, global_step=VOCAB_SIZE) # close the session sess.close() . .",
            "url": "https://rohetoric.github.io/text-vector-visualisation/jupyter/tensorflow/python/2020/05/21/tb1vis.html",
            "relUrl": "/jupyter/tensorflow/python/2020/05/21/tb1vis.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Generating Word Vectors Using FastText Blog-Post",
            "content": ". import re import string import fasttext import pandas as pd from spacy.lang.en import English from spacy.lang.en.stop_words import STOP_WORDS nlp = English() . . The dataset can be downloaded through: . Terminal: gsutil cp gs://dataset-uploader/bbc/bbc-text.csv . OR . | Visiting this website from the browser. . | . file = pd.read_csv(&quot;bbc-text.csv&quot;) # reading the dataset . df = file . df . category text . 0 | tech | tv future in the hands of viewers with home th... | . 1 | business | worldcom boss left books alone former worldc... | . 2 | sport | tigers wary of farrell gamble leicester say ... | . 3 | sport | yeading face newcastle in fa cup premiership s... | . 4 | entertainment | ocean s twelve raids box office ocean s twelve... | . ... | ... | ... | . 2220 | business | cars pull down us retail figures us retail sal... | . 2221 | politics | kilroy unveils immigration policy ex-chatshow ... | . 2222 | entertainment | rem announce new glasgow concert us band rem h... | . 2223 | politics | how political squabbles snowball it s become c... | . 2224 | sport | souness delight at euro progress boss graeme s... | . 2225 rows × 2 columns . df.drop(columns=[&quot;category&quot;], inplace=True) # don&#39;t need the labels for the text . df . text . 0 | tv future in the hands of viewers with home th... | . 1 | worldcom boss left books alone former worldc... | . 2 | tigers wary of farrell gamble leicester say ... | . 3 | yeading face newcastle in fa cup premiership s... | . 4 | ocean s twelve raids box office ocean s twelve... | . ... | ... | . 2220 | cars pull down us retail figures us retail sal... | . 2221 | kilroy unveils immigration policy ex-chatshow ... | . 2222 | rem announce new glasgow concert us band rem h... | . 2223 | how political squabbles snowball it s become c... | . 2224 | souness delight at euro progress boss graeme s... | . 2225 rows × 1 columns . refined_string_list = [] token_list = [] filtered_list = [] for query in df[&quot;text&quot;]: # removing punctuations from string string_translate = query.translate( str.maketrans(&quot;&quot;, &quot;&quot;, string.punctuation) ) # initialise string to english langauge functions of spacy spacy_doc = nlp( string_translate ) # appending empty list with tokenised string for token in spacy_doc: token_list.append(token.text) # checking if tokenised word exists in a given list of stopwords obtained for word in token_list: lexeme = nlp.vocab[ word ] if lexeme.is_stop == False: filtered_list.append( word ) # converting list of tokenised words without stopwords to sentence filtered_sentence = &quot; &quot;.join( filtered_list ) # removing multiple spaces from the string filtered_sentence = re.sub( &quot; +&quot;, &quot; &quot;, filtered_sentence ) # appending the list with strings without stop words refined_string_list.append( filtered_sentence ) # reinitialising the lists token_list = [] filtered_list = [] refined_string_list[0] . &#39;tv future hands viewers home theatre systems plasma highdefinition tvs digital video recorders moving living room way people watch tv radically different years time according expert panel gathered annual consumer electronics las vegas discuss new technologies impact favourite pastimes leading trend programmes content delivered viewers home networks cable satellite telecoms companies broadband service providers rooms portable devices talkedabout technologies ces digital personal video recorders dvr pvr settop boxes like s tivo uk s sky system allow people record store play pause forward wind tv programmes want essentially technology allows personalised tv builtin highdefinition tv sets big business japan slower europe lack highdefinition programming people forward wind adverts forget abiding network channel schedules putting alacarte entertainment networks cable satellite companies worried means terms advertising revenues brand identity viewer loyalty channels leads technology moment concern raised europe particularly growing uptake services like sky happens today months years time uk adam hume bbc broadcast s futurologist told bbc news website likes bbc issues lost advertising revenue pressing issue moment commercial uk broadcasters brand loyalty important talking content brands network brands said tim hanlon brand communications firm starcom mediavest reality broadband connections anybody producer content added challenge hard promote programme choice means said stacey jolna senior vice president tv guide tv group way people find content want watch simplified tv viewers means networks terms channels leaf google s book search engine future instead scheduler help people find want watch kind channel model work younger ipod generation taking control gadgets play suit panel recognised older generations comfortable familiar schedules channel brands know getting want choice hands mr hanlon suggested end kids diapers pushing buttons possible available said mr hanlon ultimately consumer tell market want 50 000 new gadgets technologies showcased ces enhancing tvwatching experience highdefinition tv sets new models lcd liquid crystal display tvs launched dvr capability built instead external boxes example launched humax s 26inch lcd tv 80hour tivo dvr dvd recorder s biggest satellite tv companies directtv launched branded dvr 100hours recording capability instant replay search function set pause rewind tv 90 hours microsoft chief bill gates announced preshow keynote speech partnership tivo called tivotogo means people play recorded programmes windows pcs mobile devices reflect increasing trend freeing multimedia people watch want want&#39; . with open(&quot;refined-bbc-text.txt&quot;, &quot;w&quot;) as f: for item in refined_string_list: f.write(&quot;%s n&quot; % item) . # &lt;--TRAINING THE MODEL BASED ON FASTTEXT--&gt; . print(fasttext.train_unsupervised.__doc__) . Train an unsupervised model and return a model object. input must be a filepath. The input text does not need to be tokenized as per the tokenize function, but it must be preprocessed and encoded as UTF-8. You might want to consult standard preprocessing scripts such as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html The input field must not contain any labels or use the specified label prefix unless it is ok for those words to be ignored. For an example consult the dataset pulled by the example script word-vector-example.sh, which is part of the fastText repository. . Default Configuration for parameters mentioned in [ ] for fasttext.train_unsupervised(): . input # training file path (required) model # unsupervised fasttext model {cbow, skipgram} [skipgram] lr # learning rate [0.05] dim # size of word vectors [100] ws # size of the context window [5] epoch # number of epochs [5] minCount # minimal number of word occurences [5] minn # min length of char ngram [3] maxn # max length of char ngram [6] neg # number of negatives sampled [5] wordNgrams # max length of word ngram [1] loss # loss function {ns, hs, softmax, ova} [ns] bucket # number of buckets [2000000] lrUpdateRate # change the rate of updates for the learning rate [100] t # sampling threshold [0.0001] verbose # verbose [2] . %%time model = fasttext.train_unsupervised(&quot;refined-bbc-text.txt&quot;, dim=300, thread=4) . CPU times: user 2min 34s, sys: 1.14 s, total: 2min 35s Wall time: 46.2 s . with open(&quot;tensorboard/metadata.tsv&quot;, &quot;w&quot;) as f: for item in model.words: f.write( &quot;%s n&quot; % item ) # writing the vocabulary words of the model to a text file . model.save_model(&quot;fasttextmodel.bin&quot;) # saving the model . .",
            "url": "https://rohetoric.github.io/text-vector-visualisation/jupyter/fasttext/python/2020/05/20/fasttext-model-train.html",
            "relUrl": "/jupyter/fasttext/python/2020/05/20/fasttext-model-train.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rohetoric.github.io/text-vector-visualisation/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rohetoric.github.io/text-vector-visualisation/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rohetoric.github.io/text-vector-visualisation/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}