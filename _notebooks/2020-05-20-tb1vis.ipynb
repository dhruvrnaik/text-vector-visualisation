{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Embeddings Using TF1 [NOT ADVISABLE]\n",
    "> Exploration and Visualisation of Word Vectors Using TensorFlow 1\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,tensorflow]\n",
    "\n",
    "The visualisation of embeddings using Tensorflow 1 is inspired from this <a href=\"http://www.insightsbot.com/visualize-word-embeddings-with-tensorflow\">blog</a>. \n",
    "\n",
    "The above mentioned blog gives out an appropriate theoretical description but their code has become <font color='red'>obsolete</font> and the required changes for the correct visualisation as well as the complete theoretical description is given in this notebook and in an updated way with TF2 blog-post.\n",
    "\n",
    "\n",
    "<blockquote>FastText uses the concept that embeddings are formed based on the sub-word approach, this method helps us to visualise and obtain misspellings of a word or different spellings of the same word.</blockquote>\n",
    "\n",
    "\n",
    "As we currently have the latest Tensorflow version installed, instead of downgrading it to previous version 1, we use the following code:\n",
    "```python\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "```\n",
    "This method helps us to get the behaviour of Tensorflow 1 in Tensorflow 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"TensorFlow 1\"](my_icons/tf1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from pathlib import PurePath\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorboard.plugins import projector\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()  # disabling v2 behaviour of tf1\n",
    "from tensorboard.plugins.projector import ProjectorConfig\n",
    "\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(\"fasttextmodel.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save files to visualise on tensorboard\n",
    "FOLDER_PATH = \"tb1files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "said\n",
      "mr\n",
      "</s>\n",
      "people\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(model.get_words()):\n",
    "    print(w)\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10891, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in the dataset\n",
    "VOCAB_SIZE = len(model.get_words())\n",
    "\n",
    "\n",
    "# size of the dimension of each word vector\n",
    "EMBEDDING_DIM = len(model.get_word_vector(w))\n",
    "\n",
    "\n",
    "# 2D numpy array initialised to store words with their vector representation\n",
    "embed = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11363645,  0.00304414,  0.00589875, ...,  0.00278742,\n",
       "         0.03564256, -0.10496949],\n",
       "       [ 0.05821591,  0.07343163, -0.06941246, ...,  0.00737938,\n",
       "         0.08668958, -0.05127012],\n",
       "       [ 0.06867523, -0.02112868, -0.02132288, ...,  0.05362611,\n",
       "         0.13982825,  0.04221647],\n",
       "       ...,\n",
       "       [ 0.16511762,  0.04439345, -0.14276202, ...,  0.02632121,\n",
       "         0.03970968,  0.03706815],\n",
       "       [ 0.09471416,  0.09356211,  0.00358974, ..., -0.0174412 ,\n",
       "         0.13414964,  0.02268019],\n",
       "       [ 0.07753251, -0.02356024, -0.05303693, ...,  0.14130574,\n",
       "         0.09740689,  0.0418443 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vector representation of each word in the 2D numpy array\n",
    "for i, word in enumerate(model.get_words()):\n",
    "    embed[i] = model.get_word_vector(word)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store the words\n",
    "tsv_file_path = FOLDER_PATH + \"/metadata.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tb1files/metadata.tsv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10891, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(tsv_file_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    for i, word in enumerate(model.get_words()):\n",
    "        f.write(word + \"\\n\")  # write the words to an external file\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_FILES_PATH = FOLDER_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Projection on Tensorboard 1 [Part 1]</h3>\n",
    "</center>                                     \n",
    "\n",
    "Steps for projection [Part 1]:\n",
    "\n",
    "1. Placeholder is created of size Vocab Size * Dimension of Embeddings.\n",
    "2. Creation of a global variable to store the placeholder values.\n",
    "3. New tensorflow session is started and the placeholder is passed the value of our array which stores the vocabulary and their respective embeddings.\n",
    "4. For saving values into variables and restoring variables from checkpoints, a saver object is instantiated and a writer object is initialised which outputs the graph.\n",
    "\n",
    "\n",
    "**Differences between TF 1 and TF 2.**\n",
    "\n",
    "\n",
    "1. In TF1, reset default graph can be directly called by the tensorflow library to clear the default graph stack      and reset the global default graph.\n",
    "```python\n",
    "   tf.reset_default_graph()```\n",
    "\n",
    "2. TF 2 doesn't have the placeholders as mentioned below: \n",
    "```python\n",
    "   X_init = tf.placeholder(tf.float32, shape=(VOCAB_SIZE, EMBEDDING_DIM), name=\"embedding\")```\n",
    "   \n",
    "It is cleared off by the disabling tf 2 behaviour defined through the import technique [already done in cell 1]:\n",
    "   \n",
    "```python\n",
    "   import tensorflow.compat.v1 as tf\n",
    "   tf.disable_v2_behavior()```\n",
    "   \n",
    "   \n",
    "   without which we would get the following error:\n",
    "  `AttributeError: module 'tensorflow' has no attribute 'placeholder'`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Placeholders\n",
    "tf.reset_default_graph()\n",
    "X_init = tf.placeholder(tf.float32, shape=(VOCAB_SIZE, EMBEDDING_DIM), name=\"embedding\")\n",
    "X = tf.Variable(X_init)\n",
    "\n",
    "\n",
    "# Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start Tensorflow Session\n",
    "sess = tf.Session()\n",
    "sess.run(init, feed_dict={X_init: embed})\n",
    "\n",
    "\n",
    "# Instance of Saver, save the graph.\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(TENSORBOARD_FILES_PATH, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Projection on Tensorboard 1 [Part 2]</h3>\n",
    "</center>\n",
    "\n",
    "Steps for projection [Part 2]:\n",
    "1. Instantiating the projector object.\n",
    "2. Assigning the file which contains the vocabulary to the embedding variable.\n",
    "3. Writing the configuration file for the projector read by the tensorboard using-\n",
    "```python\n",
    "projector.visualize_embeddings(writer, config)```\n",
    "4. Saving the checkpoint and closing the connection.\n",
    "\n",
    "\n",
    "Here both the projector imports are important [already imported in cell 1] i.e. \n",
    "\n",
    "```python\n",
    "from tensorboard.plugins import projector\n",
    "from tensorboard.plugins.projector import ProjectorConfig\n",
    "```\n",
    "as `visualize_embeddings()` function is defined under `projector` and we need `ProjectorConfig()` in creation for  the configuration file of the projector.\n",
    "\n",
    "If the projector is imported in the following way:\n",
    "```python\n",
    "from tensorboard.plugins import projector\n",
    "config = projector.ProjectorConfig()\n",
    "```\n",
    "the error received would be `AttributeError: module 'tensorboard.plugins.projector' has no attribute 'ProjectorConfig'`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a Tensorflow Projector\n",
    "config = ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.metadata_path = \"metadata.tsv\"\n",
    "\n",
    "# Write a projector_config\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "\n",
    "# save a checkpoint\n",
    "saver.save(sess, TENSORBOARD_FILES_PATH + \"/model.ckpt\", global_step=VOCAB_SIZE)\n",
    "\n",
    "\n",
    "# close the session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
