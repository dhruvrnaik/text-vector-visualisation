{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Word Vectors Using TF2 [Advisable]\n",
    ">Exploration and Visualisation of Word Vectors Using TensorFlow 2\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,tensorflow,python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow released their latest version of <a href=\"https://www.youtube.com/watch?v=k5c-vg4rjBw\">TensorFlow 2</a> on September 30, 2019.\n",
    "\n",
    "As also mentioned in <font color='red'>tb1vis.ipynb</font> the reason we are visualising FastText is because:\n",
    "\n",
    "<blockquote>\n",
    "FastText uses the concept that embeddings are formed based on the sub-word approach, this method helps us to visualise and obtain misspellings of a word or different spellings of the same word.</blockquote>\n",
    "\n",
    "I couldn't find any blogs on the Internet that have covered or updated their code which describes the visualisation of embeddings through the latest version of Tensorflow. \n",
    "\n",
    "Although I did take the help of an issue <a href=\"https://github.com/tensorflow/tensorboard/issues/2471 \">TF 2.0 API for using the embedding projector</a> raised on the Tensorflow repository and have come to a concluding notebook suiting my goal i.e. to visualise FastText embeddings using TF2.  \n",
    "\n",
    "The tensorflow version used in this notebook is version 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/tf2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from pathlib import PurePath\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from tensorboard.plugins import projector\n",
    "from tensorboard.plugins.projector import ProjectorConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "\n",
    "# load pre-trained fasttext model\n",
    "model = fasttext.load_model(\"fasttextmodel.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "said\n",
      "mr\n",
      "</s>\n",
      "people\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(model.get_words()):\n",
    "    print(w)\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10891, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "\n",
    "# number of words in the dataset\n",
    "VOCAB_SIZE = len(model.get_words())\n",
    "\n",
    "\n",
    "# size of the dimension of each word vector\n",
    "EMBEDDING_DIM = len(model.get_word_vector(w))\n",
    "\n",
    "\n",
    "# 2D numpy array initialised to store words with their vector representation\n",
    "embed = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "embed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11363645,  0.00304414,  0.00589875, ...,  0.00278742,\n",
       "         0.03564256, -0.10496949],\n",
       "       [ 0.05821591,  0.07343163, -0.06941246, ...,  0.00737938,\n",
       "         0.08668958, -0.05127012],\n",
       "       [ 0.06867523, -0.02112868, -0.02132288, ...,  0.05362611,\n",
       "         0.13982825,  0.04221647],\n",
       "       ...,\n",
       "       [ 0.16511762,  0.04439345, -0.14276202, ...,  0.02632121,\n",
       "         0.03970968,  0.03706815],\n",
       "       [ 0.09471416,  0.09356211,  0.00358974, ..., -0.0174412 ,\n",
       "         0.13414964,  0.02268019],\n",
       "       [ 0.07753251, -0.02356024, -0.05303693, ...,  0.14130574,\n",
       "         0.09740689,  0.0418443 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vector representation of each word in the 2D numpy array\n",
    "for i, word in enumerate(model.get_words()):\n",
    "    embed[i] = model.get_word_vector(word)\n",
    "embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store the words\n",
    "tsv_file_path = \"tensorboard/metadata.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10891, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide output\n",
    "with open(tsv_file_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    for i, word in enumerate(model.get_words()):\n",
    "        f.write(word + \"\\n\")  # write the words to an external file\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection on Tensorboard 2\n",
    "                                  \n",
    "\n",
    "Steps for projection:\n",
    " \n",
    "1. Define the function `register_embedding()` and `save_label_tsv()` to configure the projector as well as save the projector configuration files and metadata file to the same folder.\n",
    "2. Initialise the path variables accordingly and call the above function with suitable path variables as shown in below cells.\n",
    "3. Creation of the tensorflow variable instead of tensorflow placeholder.\n",
    "4. A saver class object is initialised and checkpoint is created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between TF 1 and TF 2\n",
    "<ul>\n",
    "<li>We cannot call the reset default graph method directly from tf library which we did for TensorFlow 1. It is invoked in TF2 as: \n",
    "\n",
    "```python\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "```\n",
    "</li>\n",
    "\n",
    "<li>There is no placeholder required here. A tf variable is created which is passed parameters. A TF variable is  shown below. Here the parameters are x: the array which contains the embeddings and name: name of the embedding file.\n",
    "\n",
    "`python\n",
    "tensor_embeddings = tf.Variable(x, name=EMBEDDINGS_TENSOR_NAME)\n",
    "`\n",
    "According to the TF2 documentation a <a href=\"https://www.tensorflow.org/guide/variable\">tensorflow variable</a> is defined as:\n",
    "\n",
    "<blockquote>\n",
    "A variable maintains shared, persistent state manipulated by a program.\n",
    "\n",
    "  The Variable() constructor requires an initial value for the variable, which\n",
    "  can be a Tensor of any type and shape. This initial value defines the type\n",
    "  and shape of the variable. After construction, the type and shape of the\n",
    "  variable are fixed. The value can be changed using one of the assign methods.\n",
    "</blockquote>\n",
    "</li>\n",
    "\n",
    "<li>There is no concept of `session` as well as `saver` in TF2 yet. To workaround this, we just use the `saver` class for the creation of checkpoints by initialising the `saver` object with the tensorflow variable and pass `None` as value to the `session` parameter in `saver.save()`.\n",
    "\n",
    "```python\n",
    "saver = tf.compat.v1.train.Saver([tensor_embeddings])  \n",
    "saver.save(sess=None, global_step=STEP, save_path=EMBEDDINGS_FPATH)\n",
    "```\n",
    "</ul>\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()  # clearing the default graph stack\n",
    "\n",
    "\n",
    "def register_embedding(\n",
    "    embedding_tensor_name: str, meta_data_fname: str, log_dir: str,\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Configuring the projector to be read by the tensorboard.\n",
    "    \n",
    "    Args:\n",
    "    embedding_tensor_name(str): embeddings file name\n",
    "    meta_data_fname(str): metadata file name\n",
    "    log_dir(str): folder where tensorboard files and the metadata file are saved\n",
    "    \n",
    "    Returns:\n",
    "    None    \n",
    "    \n",
    "    \"\"\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_tensor_name\n",
    "    embedding.metadata_path = meta_data_fname\n",
    "    projector.visualize_embeddings(\n",
    "        log_dir, config\n",
    "    )  # storing the configuration files of projector where tensorboard files are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels_tsv(labels: list, filepath: str, log_dir: str,) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Storing the vocabulary of words in the dataset to a file\n",
    "    \n",
    "    Args:\n",
    "    labels: vocabulary i.e. words in the dataset\n",
    "    filepath: metadata file name\n",
    "    log_dir: \"folder where tensorboard files and projector files are saved\n",
    "    \n",
    "    Returns:\n",
    "    None  \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    with open(PurePath(log_dir, filepath), \"w\") as f:\n",
    "        for label in labels:\n",
    "            f.write(\"{}\\n\".format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"tb2files\"  # folder which will contain all the tensorboard log files\n",
    "\n",
    "# Labels i.e. the words in the dataset will be stored in this file\n",
    "META_DATA_FNAME = \"meta.tsv\"\n",
    "\n",
    "# name of the file which will have the embeddings stored\n",
    "EMBEDDINGS_TENSOR_NAME = \"embeddings\"\n",
    "\n",
    "# path for checkpoint of the saved embeddings\n",
    "EMBEDDINGS_FPATH = PurePath(LOG_DIR, EMBEDDINGS_TENSOR_NAME + \".ckpt\")\n",
    "STEP = 0\n",
    "\n",
    "\n",
    "x = embed  # array containing the embeddings\n",
    "y = model.get_words()  # list containing the vocabulary\n",
    "register_embedding(EMBEDDINGS_TENSOR_NAME, META_DATA_FNAME, LOG_DIR)\n",
    "save_labels_tsv(y, META_DATA_FNAME, LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_embeddings = tf.Variable(\n",
    "    x, name=EMBEDDINGS_TENSOR_NAME\n",
    ")  # creation of the tensorflow variable, x: array which contains the embeddings,\n",
    "# name: name of the file which will have the embeddings stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tb2files/embeddings.ckpt-0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "\n",
    "saver = tf.compat.v1.train.Saver(\n",
    "    [tensor_embeddings]\n",
    ")  # Tensorflow variable passed as argument for saver object to be initialised\n",
    "saver.save(\n",
    "    sess=None, global_step=STEP, save_path=EMBEDDINGS_FPATH\n",
    ")  # saving the checkpoint for the embedding files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
